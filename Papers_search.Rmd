---
title: "Papers_search"
author: "ML"
date: "22 January 2019"
output: html_document
---


```{r setup, cache = F, echo=FALSE, results = FALSE}
knitr::opts_chunk$set(error = TRUE) #allow some execution errors for demonstration purposes
knitr::opts_chunk$set(eval = TRUE, echo = TRUE, warning = FALSE, collapse = TRUE, comment = "#>")
sessionInfo()
```

```{r prepare, message = FALSE, echo = FALSE, eval = FALSE, warning = FALSE}
install.packages("diagram")
```

```{r load packages, message = FALSE, echo = FALSE, eval = TRUE, warning = FALSE}
library(diagram)
```

# EXERCISES - SEARCHING AND SCREENING PAPERS FOR META-ANALYSIS  

Prepared by Malgorzata (Losia) Lagisz, BEES, UNSW, AU, losialagisz@gmail.com.   

**Setup:** Work in pairs or small groups. R/Rstudio and Internet connection needed.   


### Purpose of the exercise:   
In this exercise you get some practice and tips on:   
  
 * Formulating the question for a meta-analysis   
 * Deciding on inclusion criteria   
 * Making decision trees   
 * Performing searches for relevant literature   
 * Screening the literature  
 * PRISMA and other documentation  
<br>

  
********************************************************************************  
### Introduction   

Imagine, you woke up in the morning feeling rather sick. You decided to work from home to get some rest a get better more quickly. You don't not feel energetic enough to crunch numbers or write, so you thought you may catch up on scientific reading instead. You pick up the book sitting on top of the "to-read"" pile and open it at a random place. It happens to be **Ecoimmunology** (edited by Gregory Demas, Randy Nelson, Randy Joe Nelson, 2012) and it opens on the following fragment:

```{r TI_book_fragment, echo=FALSE, fig.cap=" ", out.width='50%'}
knitr::include_graphics("./images/TI_book_fragment.png")
```

You find it intriguing and start thinking this might be something (a topic) you could do a meta-analysis on.  *(Ok, this is a hypothetical example of coming up with the topic for a meta-analysis, but it sometimes happens - you read or hear some interesting questions and it sparks desire to put some numbers on some pattern...)*   

Let's use this hypothetical topic for the purpose of the exercise. We hope the topic is quite appealing and quiet easy to understand (we all get sick and most of us reproduce, or at least try to).

In this exercise, we will use a few basic tools to illustrate how to perform the first few steps of meta-analysis: refining the question, searching and screening literature (these are the same steps as in Systematic Reviews, so the tools are the same and can be usually described as systematic literature/evidence reviewing tools).  
Note that some R packages (and many other online/software tools) are available (and more always coming) that can help with some of the tasks during this exercise, but there is no single "perfect" tool for the whole process usable for all disciplines and types of questions (although some attempts have been made). We simply do not have enough time to introduce more tools and the main purpose of this exercise is to familiarize you with the basic principles/issues that apply when you use any tools. You can always try alternative tools in your own time.   

**Note:** There will be some **QUESTIONS** to answer and **TASKS** to be performed, as well as **RESOURCES**      
There is no *R* code to be run in this exercise.   
<br>


********************************************************************************
###  Formulating the question for a meta-analysis  

Topic is usually broad and vague area of interest, for a meta-analysis we need a much more concrete question.
Generally, the more focused the question, the easier and quicker it is to perform the meta-analysis, but the answer becomes less general and there might be less evidence available.

**QUESTION 0:**  
What is your initial feeling about the meta-analytical question that can be derived from the text fragment cited above?   


**QUESTION 1:**  
What are at least 2 ways of improving this initial question to make it more amenable for a meta-analysis?

These question refinements are leading us gently into deciding which studies to use in a meta-analysis. However, before we move onto inclusion criteria, it is wise to gather more intelligence (Unless you are already an expert on the topic).

**QUESTION 2:**  
What should we do now?    
 a) Check if there already is a good recent meta-analysis on the topic.  
 b) Find and carefully read key reviews on the topic.  
 c) Find and carefully read a few representative empirical papers.  
 d) Further refine the question.  
 e) All of the above.  


**TASK 1:**  
Are there any meta-analyses (or similar) on terminal investment in animals?   (you can skip the following few tasks if short of time during the exercise)   
In Google Scholar (https://scholar.google.com) run:  
*"meta-analytic" OR "meta-analysis" "terminal investment"*   
(you can paste this into search box or use Advanced search; see instructions and other Google Scholar tips here: https://guides.library.ucsc.edu/c.php?g=745384&p=5361954 and https://scholar.google.com/intl/en/scholar/help.html).   

You should get around 300 results. By skimming first few pages you can see a few meta-analyses on stress, diet, senescence, parasitism etc., but none probably specifically on terminal investment. There are many empirical papers coming up in this search because they mention meta-analysis in their body text or reference list - note that Google Scholar searches full text by default. Also by default, the returned results are arranged in order of relevance to the search (whatever that means - its a complex algorithm that decides) and a few first pages contain some potentially relevant empirical papers.    

**TASK 2:**   
You can restrict the search to titles only using:  
*allintitle: "meta analytic" OR "meta analysis" "terminal investment"*   
Now there are no matching hits; but there is no option for searching just in titles and abstracts.    

Other things you should know about Google Scholar - its search interface is rather simplistic and restrictive, it shows only up to 1,000 results for any particular search query, ranking algorithms are obscured, its hard to export the references, the literature coverage may fluctuate (its based on web scraping) and the searches may not be reproducible. Because of this it is not really recommended for the main searches in systematic reviews. On the upside, it can be helpful for additional searches and finding grey literature (not published in peer-review journals).  

**RESOURCES:**   
Other software useful for exploratory (scoping) ad additional searches (check them up later, also note that they may have similar disadvantages to these of Google Scholar):
 * Semantic Scholar: https://www.semanticscholar.org/
 * Dimensions: https://www.dimensions.ai/
 * CiteHero: https://citehero.com/

**TASK 3:**   
Run another Google Scholar quick search:  
*"terminal investment" reproduction*   
This time you should get around 1900 records. Note that Google Scholar does so called Automated Query Expansion for terms that are not set to be exact phrases (in quotes) - it uses alternative forms of these terms.  

**TASK 4:**   
Try Google Scholar quick search:   
*"terminal investment" "immune challenge"*  
This time you should get around 400 records. Skim through the first three pages of the results to get a feel of the diversity of the studies found (e.g. what taxa are present and what they might measure as reproductive investment) and note the wording of their titles.  

**QUESTION 3:**  
In medical sciences, most meta-analyses (and systematic reviews) fit into the PICO (or related) question framework.   

PICO stands for:   
  * P = Population   
  * I = Intervention  
  * C = Comparison/Control group   
  * O = Outcome   

Formulate meta-analytic question "whether immune challenge can result in terminal investment" using PICO terms (actually this fits really well because we can use experimental data).   


**RESOURCES:**   
An extensive list of alternative systematic review formulation question frameworks here:  https://www.networks.nhs.uk/nhs-networks/nwas-library-and-information-service/documents/alternative-question-structures-for-different-types-of-systematic-review    )


**Note:** you should always be able to justify your question as important, relevant and timely. 
<br>


********************************************************************************
###  Deciding on inclusion criteria   

The basic PICO components are usually not sufficient to perform the effective screening of the papers for inclusion in meta-analysis.
There are additional both technical and biological refinements to be considered for inclusion criteria in our project.
Most importantly, you always need to be able to justify your inclusion criteria!  

**QUESTION 4:**  
Time-span: should we include studies from any year?  

**QUESTION 5:**  
Language: should we include studies published in any language?  

**Note:** publication status can be also used as a criterium - are you happy to include some types of grey literature (e.g. conference proceedings, theses)?

**QUESTION 6:**  
Study type: should we include any type of study?  

**QUESTION 7:**  
Taxa: should we include any type of taxa?   

**QUESTION 8:**   
Manipulation type: should we include any immune challenge?   

**QUESTION 9:**   
Control group: should we accept any type of control group?  

**QUESTION 10:**   
Outcome type: should we accept any reproduction-related measurements and what that means?   

**QUESTION 11:**  
Data type: should we accept any type of data for meta-analysis?   
<br>


********************************************************************************
### Making decision trees   

Good decision trees make screening the literature faster and more reliable.  
Briefly, if the initial (most general) questions, representing screening criteria, are not fulfilled, the study gets excluded and there is no need to evaluate the remaining criteria, so we save some time.  

**QUESTION 12:**  
How many stages there are during literature screening process?   

**QUESTION 13:**  
How many decision trees do we need, and why, for the literature screening process?   

An example decision tree for abstract screening:  

```{r diagram decision tree abstracts, fig.width=10, fig.height=6, echo=FALSE, message=FALSE}
par(mar=c(1,1,1,1))
openplotmat() #create an empyty plot
elpos <- coordinates (pos = c(2, 2, 2, 2, 2))
fromto <- matrix(ncol = 2, byrow = TRUE, data = c(1,2, 1,3, 3,4, 3,5, 5,6, 5,7, 7,8, 7,9))
nr <- nrow(fromto)
arrpos <- matrix(ncol = 2, nrow = nr)
for (i in 1:nr) (arrpos[i, ] <- straightarrow (to = elpos[fromto[i, 2], ], from = elpos[fromto[i, 1], ], lwd = 2, arr.pos = 0.7, arr.length = 0.4, arr.type = "simple"))

textrect (mid = elpos[1,], radx = 0.15, rady = 0.05, lwd = 1, lab = c("Study on non-human","multicellular animal?"), box.col = "white", shadow.col = "grey", shadow.size = 0, cex = 0.6)

textellipse (mid = elpos[2,], radx = 0.15, rady = 0.05, lwd = 1, lab = "EXCLUDE", box.col = "coral1", shadow.col = "grey", shadow.size = 0, cex = 0.8)

textrect (mid = elpos[3,], radx = 0.15, rady = 0.05, lwd = 1, lab = c("Experimental study?"), box.col = "white", shadow.col = "grey", shadow.size = 0, cex = 0.6)

textellipse (mid = elpos[4,], radx = 0.15, rady = 0.05, lwd = 1, lab = "EXCLUDE", box.col = "coral1", shadow.col = "grey", shadow.size = 0.001, cex = 0.8)

textrect (mid = elpos[5,], radx = 0.15, rady = 0.05, lwd = 1, lab = c("Experimental","immune challenge?"), box.col = "white", shadow.col = "grey", shadow.size = 0, cex = 0.6)

textellipse (mid = elpos[6,], radx = 0.15, rady = 0.05, lwd = 1, lab = "EXCLUDE", box.col = "coral1", shadow.col = "grey", shadow.size = 0, cex = 0.8)

textrect (mid = elpos[7,], radx = 0.15, rady = 0.05, lwd = 1, lab = c("Reproduction-related","traits measured?"), box.col = "white", shadow.col = "grey", shadow.size = 0, cex = 0.6)

textellipse (mid = elpos[8,], radx = 0.15, rady = 0.05, lwd = 1, lab = "EXCLUDE", box.col = "coral1", shadow.col = "grey", shadow.size = 0, cex = 0.8)

textellipse (mid = elpos[9,], radx = 0.15, rady = 0.05, lwd = 1, lab = c("INCLUDE", "for full-text screening"), box.col = "light green", shadow.col = "grey", shadow.size = 0, cex = 0.8)

text(arrpos[1, 1] - 0.1, arrpos[1, 2] + 0.02, "no")
text(arrpos[2, 1] + 0.07, arrpos[2, 2] + 0.03, "yes/probably")
text(arrpos[3, 1] - 0.1, arrpos[3, 2] + 0.02, "no")
text(arrpos[4, 1] + 0.07, arrpos[4, 2] + 0.03, "yes/probably")
text(arrpos[5, 1] - 0.1, arrpos[5, 2] + 0.02, "no")
text(arrpos[6, 1] + 0.07, arrpos[6, 2] + 0.03, "yes/probably")
text(arrpos[7, 1] - 0.1, arrpos[7, 2] + 0.02, "no")
text(arrpos[8, 1] + 0.07, arrpos[8, 2] + 0.03, "yes/probably")
```


**QUESTION 14:**  
Why in the abstract screening decision tree there is no question about the type of data used to express measurements reported in the study?   

Full-text screening is more strict and we want to have answers to all our inclusion criteria. In principle we only have "yes" and "no" answers to the screening questions. However, in practice, if a study looks very relevant but some information is ambiguous or missing, you may decide to contact the authors to get some extra information or data. in such case, you can provisionary include such papers and make final decision once you get, or not, the replies.   

An example decision tree for full-text screening:    

```{r diagram decision tree fulltexts, fig.width=10, fig.height=6, echo=FALSE, message=FALSE}
par(mar=c(1,1,1,1))
openplotmat() #create an empyty plot
elpos <- coordinates (pos = c(2, 2, 2, 2, 2, 2))
fromto <- matrix(ncol = 2, byrow = TRUE, data = c(1,2, 1,3, 3,4, 3,5, 5,6, 5,7, 7,8, 7,9, 9,10, 9,11 ))
nr <- nrow(fromto)
arrpos <- matrix(ncol = 2, nrow = nr)
for (i in 1:nr) (arrpos[i, ] <- straightarrow (to = elpos[fromto[i, 2], ], from = elpos[fromto[i, 1], ], lwd = 2, arr.pos = 0.7, arr.length = 0.4, arr.type = "simple"))

textrect (mid = elpos[1,], radx = 0.15, rady = 0.05, lwd = 1, lab = c("Study on non-human","multicellular animal?"), box.col = "white", shadow.col = "grey", shadow.size = 0, cex = 0.6)

textellipse (mid = elpos[2,], radx = 0.15, rady = 0.05, lwd = 1, lab = "EXCLUDE", box.col = "coral1", shadow.col = "grey", shadow.size = 0, cex = 0.8)

textrect (mid = elpos[3,], radx = 0.15, rady = 0.05, lwd = 1, lab = c("Experimental study?"), box.col = "white", shadow.col = "grey", shadow.size = 0, cex = 0.6)

textellipse (mid = elpos[4,], radx = 0.15, rady = 0.05, lwd = 1, lab = "EXCLUDE", box.col = "coral1", shadow.col = "grey", shadow.size = 0.001, cex = 0.8)

textrect (mid = elpos[5,], radx = 0.15, rady = 0.05, lwd = 1, lab = c("Experimental","immune challenge?"), box.col = "white", shadow.col = "grey", shadow.size = 0, cex = 0.6)

textellipse (mid = elpos[6,], radx = 0.15, rady = 0.05, lwd = 1, lab = "EXCLUDE", box.col = "coral1", shadow.col = "grey", shadow.size = 0, cex = 0.8)

textrect (mid = elpos[7,], radx = 0.15, rady = 0.05, lwd = 1, lab = c("Reproduction-related","traits measured?"), box.col = "white", shadow.col = "grey", shadow.size = 0, cex = 0.6)

textellipse (mid = elpos[8,], radx = 0.15, rady = 0.05, lwd = 1, lab = "EXCLUDE", box.col = "coral1", shadow.col = "grey", shadow.size = 0, cex = 0.8)

textrect (mid = elpos[9,], radx = 0.15, rady = 0.05, lwd = 1, lab = c("Reported means with SD (or SE)", "and N (sample sizes) for the treatment", "and control groups or test statistics", "from which effect sizes for the difference","between two groups can be calculated?"), box.col = "white", shadow.col = "grey", shadow.size = 0, cex = 0.6)

textellipse (mid = elpos[10,], radx = 0.15, rady = 0.05, lwd = 1, lab = "EXCLUDE", box.col = "coral1", shadow.col = "grey", shadow.size = 0, cex = 0.8)

textellipse (mid = elpos[11,], radx = 0.15, rady = 0.05, lwd = 1, lab = c("INCLUDE", "for meta-analysis"), box.col = "light green", shadow.col = "grey", shadow.size = 0, cex = 0.8)

text(arrpos[1, 1] - 0.1, arrpos[1, 2] + 0.02, "no")
text(arrpos[2, 1] + 0.05, arrpos[2, 2] + 0.03, "yes")
text(arrpos[3, 1] - 0.1, arrpos[3, 2] + 0.02, "no")
text(arrpos[4, 1] + 0.05, arrpos[4, 2] + 0.03, "yes")
text(arrpos[5, 1] - 0.1, arrpos[5, 2] + 0.02, "no")
text(arrpos[6, 1] + 0.05, arrpos[6, 2] + 0.03, "yes")
text(arrpos[7, 1] - 0.1, arrpos[7, 2] + 0.02, "no")
text(arrpos[8, 1] + 0.05, arrpos[8, 2] + 0.03, "yes")
text(arrpos[9, 1] - 0.1, arrpos[9, 2] + 0.02, "no")
text(arrpos[10, 1] + 0.05, arrpos[10, 2] + 0.03, "yes")
```


**QUESTION 15:**  
What question is missing on the top of the full-text decision tree?   
(Tip: its not about the Language or publication year)   
<br>

     
********************************************************************************
###  Performing searches for relevant literature  

Normally, we attempt to do a fairly comprehensive literature search (a full systematic review). The reason is that we want to have as many relevant data points as possible to increase the power of statistical analyses. The other reason is that we make effort to capture obscure studies that potentially are more likely to report non-significant results, and this way we actively try to reduce publication bias in our data set. Search strategy should include using at least two wide-coverage/interdisciplinary databases (we recommend Scopus and Web of Science), and a few additional sources of literature such as forward and backward reference searching (looking at cited and citing papers), publication lists of the key authors or labs, searching for unpublished studies and data sets (we will not cover these here).   

In this exercise we only do a bit of the core search using 2 databases:
Scopus and Web of Science.

**RESOURCES:** 
Scopus: https://www.scopus.com/   
Web of Science (WoS): http://apps.webofknowledge.com   
**Note:** WoS coverage depends of institutional subscription package purchased, so results of searches can vary among institutions. Also searching just core collection vs. all available databases can make a difference.     

**TASK 5:**   
Using Scopus **Advanced Search** (you can read more about the syntax here: https://dev.elsevier.com/tips/ScopusSearchTips.htm) run this search strings:  
 1. *TITLE-ABS-KEY ( "terminal investment"  AND  "immune challenge" )*    
(approx. 14 hits - search terms too restrictive)   
 2. *TITLE-ABS-KEY ( ( "terminal investment"  OR  "reproductive effort" )  AND  ( "immune challeng\*"  OR  "immunochalleng\*" ) )  *
(approx. 25 hits - a bit better, but still too restrictive)   
 3. *TITLE-ABS-KEY ( ( "terminal investment"  OR  "reproductive effort" )  AND  ( "immune challeng\*"  OR  "immunochalleng\*"  OR  "infect\*" ) )  *
(approx. 180 hits - much better number, but we probably now have included many non-experimental studies while still missing relevant ones)    
 4. *( TITLE-ABS-KEY ( ( "terminal investment"  OR  "reproductive effort" )  AND  ( "immune challeng\*"  OR  "immunochalleng\*"  OR  "infect\*" ) )  AND NOT  TITLE-ABS-KEY ( load  OR  human  OR  people ) )* 
(approx. 144 hits - by adding a group of exclusion terms (AND NOT) we probably got rid of some irrelevant ones).  

Ideally, we should think and test some more terms for inclusion, after doing some scoping searches, reading reviews and sentinel relevant empirical papers. We stop for now and assume that the string above is our final search string. Note that Scopus Advanced Search stores all your searches ans search strings in a neat table at the bottom of the search window - its easy to copy this out and include in the documentation on how you developed your search strategy (same applies to WoS database; you need to register in Scopus and WoS to save searches and alerts across multiple search sessions). The \* symbol stands for wildcard and indicates "any symbols" since these databases do not do Automatic Query Expansion.    

**TASK 6:**    
We can easily translate our final search string from Scopus into Web of Science (WoS) **Advanced Search** (you can read more about the syntax here: https://images.webofknowledge.com/images/help/WOS/hs_search_operators.html). Run this search string:    
*TS = (( ( "terminal investment" OR "reproductive effort" ) AND ( "immune challeng\*"  OR  "immunochalleng\*"  OR  "infect\*"  ) ) NOT ( load OR human OR people ) )*  
(approx. 249 hits - more than from Scopus).   WoS stores your search history and also which sub-data bases were included in the search: Indexes=SCI-EXPANDED, SSCI, A&HCI, CPCI-S, CPCI-SSH, BKCI-S, BKCI-SSH, ESCI, CCR-EXPANDED, IC Timespan=All years" (you can restrict these in Advanced Search)

Both Scopus and WoS allow exporting the found records (with limitations on the max. number per export, **remember to tick the boxes for exporting abstracts!**) - there are multiple file formats you can use, but .ris and .bib are most commonly used ones- you will need to figure out which ones work for your reference manager software. The next step would be downloading all found records from your "final" search into reference manager of your choice. Combine the references from two databases into a single project/folder and removing duplicates (most reference managers have such function, but it never works perfectly - do a visual check after sorting by titles!) before proceeding to screening stage. An R package *revtools* (https://cran.rstudio.com/web/packages/revtools/ and http://revtools.net/) can act as a reference manager and also has a good de-dupiction function.      

**Note:** It is also a good time to write up your meta-analysis protocol and preregistering your meta-analysis (e.g. on OSF)!
<br>


********************************************************************************
###  Screening the literature   
 
Theoretically you could do screening in the database browser or your reference manager, but since it is recommended that 2 people do the screening independently (or at least part of the screening), it is good to use a dedicated tool to manage this process.  

**RESOURCES:** 
Rayyan: https://rayyan.qcri.org/welcome (free, you can use it in the web browser or as a phone app; for more details see: http://libraryguides.mcgill.ca/rayyan/home)  
Abstrackr:  http://abstrackr.cebm.brown.edu/account/login (another free tool)

**Note:* it is possible to deduplication algorithms in these are not very good, so its advisable to deduplicate and check your records in the reference manager before uploading to the screening software.    


**TASK 7:**  
We prepared a file (*Scopus10.ris*) with 10 references sampled from the set found using search string (these are sets of references from *( ( "terminal investment"  OR  "reproductive effort" )  AND  ( "immune challeng\*"  OR  "immunochalleng\*" ) )  *) in Scopus (same references came up in the equivalent search in WoS).    

**Note:** Sometimes the biblio-files with the same extension differ a bit in structure and reference managers (and Rayyan) may have problem opening them. Getting your files into exactly right format can sometimes be frustrating.   

We will use these ten references to practice title and abstract screening in Rayyan. Screening can be also done in Abstracker, but we found Rayyan bit more reliable and more user-friendly.   
If you are working in pairs or small groups, one person should create a new project in Rayyan (see below) and invite another person/people to the project using their email address.  

 1. Open Rayyan (we assume you already created a free account) and log in.    
 2. My Reviews -> New REVIEW...   
 3. Enter project title, e.g. "Terminal Investment"" and description, e.g. "test"."   
 4. Select files -> upload *Scopus10.ris* and press Continue.     
It may take several minutes for larger files and slower connections, but eventually the uploaded references should appear in the left panel under "Search methods" (you can upload more files with references via "Add new" link). Click on the "Uploaded References [scopus10.ris]" link in this panel to see the references in the right panel. Generally clicking on the links within panels on the left will filter the records in the right panels to include on highlighted sets (filtering, to get rid of the filter click on it again to remove highlighting).  
In the left panels there are several sub-panels.  
 5. In the "Keywords for include"" you can add (via "Add new" link) following terms (or at least a few of these): challenge, challenged, injected, injection, infected, infection, PHA, SRBC, vaccine (these terms will be highlighted green in the titles and abstracts to help them find more easily; Rayyan tries to populate this list by itself - but its algorithm is medical-centered so you should adjust this list for your projects).   
 6. "Keywords for exclude" - " set to: review, reviewed, model, human, people, women, men (these terms will be highlighted red in the titles and abstracts to help them find more easily; Rayyan tries to populate this list by itself - but its algorithm is medical-centered so you should adjust this list for your projects).    
 7. Click on "All reviews" button in the top right corner to go to the project lists, then open your=project again by choosing it from the list. A view with a pie chart (your screening progress) and a few buttons will appear. If you need to invite other people to do screening in parallel with you, do that by clicking on the "Invite" button. Stick with "Collaborator" selection and enter the email and message (e.g. "help me!"). Open your project again - you should see your collaborators added with some info on their work so far. Press on "Show" button to see the references view.  
 8. Click on the first review from the top - an abstract and other record info will appear in the bottom panel.  
 9. Using pour abstract screening decision tree decide whether include this paper for the next stage of screening and press "Include" or "Exclude" button accordingly. And do the assessment for the remaining records. As you go you can create and assign new labels to individual records, e.g.: "unsure/check". You could also enter reasons for exclusion, e.g. "human" (we normally don't do this as there are often hundreds or thousands excluded records per project at this stage).  
10. Go to "All reviews" and check if other people finished their screening, if applicable. After that you can lift the "blind" mode, export all the records with decisions into a .csv files. For collaborative screens Rayyan will now provide filters to display all consensus included, consensus excluded and records with conflicting decisions. If you have any "conflicts" have a look again and discuss why you couldn't agree on a given record , come to agreement and make final decision. Export the records with final consensus decisions, for documentation.   


**QUESTION 16:** 
How many papers you excluded? How many conflicts you got if you collaboratively screened (and if so what was causing these conflicts, can the decision tree be improved?)


**TASK 8:**  
Its time to do the full-text screening for the papers that passed title and abstract screening. There are some full-text papers that passed the initial screening in the directory - assess Nielsen2012 and Ardia2005 using our full-text screening decision tree. Hint: dont read the whole paper - just try to skim it and look for the key information.      

**QUESTION 17:**  
What was your decision about Nielsen2012 and Ardia2005 papers?   
<br>


********************************************************************************
###  PRISMA and other documentation  

It is very important to keep detailed record of the whole searching, screening and data extraction process. Searching and screening are usually represented as some variant of PRISMA diagram.
With our 144 records from Scopus, 249 from WoS (these two databases are our "main search"), and imaginary around 400 records from other searches we might have done if we had more time, we would probably end up with something like this one:    


```{r diagram decision tree PRISMA, fig.width=10, fig.height=6, echo=FALSE, message=FALSE}
par(mar=c(1,1,1,1))
openplotmat() #create an empyty plot
elpos <- coordinates (pos = c(3, 3, 4, 3, 4, 3, 4, 3, 4, 4))
#text(elpos) #temporary label element positions with numbers

elpos[19,] <- elpos[19,] + c(0.02,0) #shift positions for the "EXCLUDED" element
elpos[26, ] <- elpos[26,] + c(0.02,0) #shift positions for the "EXCLUDED" element

#draw arrows
fromto1 <- matrix(ncol = 2, byrow = TRUE, data = c(1,8, 2,8, 8,15, 15,22, 22,29, 29,33, 3,13, 13,20, 20,27, 27,33, 15,19, 22,26)) #first set of arrows
arrpos1 <- matrix(ncol = 2, nrow = nrow(fromto1)) #set arrow psitions
for (i in 1:nrow(fromto1)) (arrpos1[i, ] <- straightarrow (to = elpos[fromto1[i, 2], ], from = elpos[fromto1[i, 1], ], lwd = 2, arr.pos = 0.8, arr.length = 0.3, arr.type = "simple")) #draw arrows

fromto2 <- matrix(ncol = 2, byrow = TRUE, data = c(13,19, 27,26)) #second set of arrows
arrpos2 <- matrix(ncol = 2, nrow = nrow(fromto2)) #set arrow psitions
for (i in 1:nrow(fromto2)) (arrpos2[i, ] <- straightarrow (to = elpos[fromto2[i, 2], ], from = elpos[fromto2[i, 1], ], lwd = 2, arr.pos = 0.65, arr.length = 0.3, arr.type = "simple")) #draw arrows

#draw rectangles and ellipses
textrect (mid = elpos[1,], radx = 0.15, rady = 0.04, lwd = 1, lab = c("SCOPUS","N = 144"), box.col = "white", shadow.col = "grey", shadow.size = 0, cex = 0.8)

textrect (mid = elpos[2,], radx = 0.15, rady = 0.04, lwd = 1, lab = c("Web of Science", "N = 249"), box.col = "white", shadow.col = "grey", shadow.size = 0, cex = 0.8)

textrect (mid = elpos[3,], radx = 0.15, rady = 0.04, lwd = 1, lab = c("other sources", "N = approx. 400"), box.col = "white", shadow.col = "grey", shadow.size = 0, cex = 0.8)

textrect (mid = elpos[8,], radx = 0.15, rady = 0.04, lwd = 1, lab = c("After duplicates removed", "N = 310"), box.col = "white", shadow.col = "grey", shadow.size = 0, cex = 0.8)

textrect (mid = elpos[15,], radx = 0.15, rady = 0.04, lwd = 1, lab = c("Deduplicated records screened", "N = 310"), box.col = "white", shadow.col = "grey", shadow.size = 0, cex = 0.8)

textrect (mid = elpos[22,], radx = 0.15, rady = 0.04, lwd = 1, lab = c("Full-text screened", "N = 50"), box.col = "white", shadow.col = "grey", shadow.size = 0, cex = 0.8)

textrect (mid = elpos[13,], radx = 0.15, rady = 0.04, lwd = 1, lab = c("Records screened", "N = approx. 400"), box.col = "white", shadow.col = "grey", shadow.size = 0, cex = 0.8)

textrect (mid = elpos[20,], radx = 0.15, rady = 0.04, lwd = 1, lab = c("Dedupicated against main search", "N = 10"), box.col = "white", shadow.col = "grey", shadow.size = 0, cex = 0.8)

textrect (mid = elpos[27,], radx = 0.15, rady = 0.04, lwd = 1, lab = c("Full-text screened", "N = 10"), box.col = "white", shadow.col = "grey", shadow.size = 0, cex = 0.8)

textellipse (mid = elpos[33,], radx = 0.15, rady = 0.05, lwd = 1, lab = c("INCLUDED", "in meta-analysis", "N = 39"), box.col = "light green", shadow.col = "grey", shadow.size = 0, cex = 0.8)

textellipse (mid = elpos[19,] + c(0.05,0), radx = 0.06, rady = 0.02, lwd = 1, lab = "EXCLUDED", box.col = "coral1", shadow.col = "grey", shadow.size = 0, cex = 0.8)

textellipse (mid = elpos[26,]+ c(0.05,0), radx = 0.06, rady = 0.02, lwd = 1, lab = "EXCLUDED", box.col = "coral1", shadow.col = "grey", shadow.size = 0, cex = 0.8)

#put numbers on exclusion arrows
text(elpos[19, 1] - 0.07, elpos[19, 2] + 0.02, "260")
text(elpos[26, 1] - 0.07, elpos[26, 2] + 0.02, "15")
text(elpos[19, 1] + 0.12, elpos[19, 2] + 0.12, "approx.380")
text(elpos[26, 1] + 0.14, elpos[26, 2] + 0.02, "6")
text(elpos[29, 1] + 0.02, elpos[29, 2] + 0.06, "35")
text(elpos[30, 1] + 0, elpos[30, 2] - 0.02, "4")
#text(arrpos[3, 1] - 0.1, arrpos[3, 2] + 0.02, "no")
```
<br>


**QUESTION 18:**   
How many papers in total were excluded during the full-text screening?   


**Note:** When you exclude records in the full-text screen keep record for each individual paper on the main reason for exclusion. Later present this information as a supplementary table for your meta-analyses. (Table of included studies should go in the main text, if possible).    

**QUESTION 19:**  
Above you can see a PRISMA (flow) diagram. The other important document(s) associated with PRISMA is a checklist (http://prisma-statement.org/documents/PRISMA%202009%20checklist.pdf). 
PRISMA checklist is a document that helps making sure that you reported the key information how a systematic review (including meta-analysis) was done.    
Have a look at the PRISMA checklist. Find and read the items that are related to what we have covered in this exercise. What are the numbers (in the # column) of these items?   

**Note:** this PRISMA was designed for meedical systematic reviews, and some items are not very relevant or useful. Also, it is due to be updated.   
<br>


*We hope you enjoyed this exercise! Remeber, we just roughly covered the basics and there is much more to learn!*       

